{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd33245d-26e7-40c9-96bb-7361405d16e0",
   "metadata": {},
   "source": [
    "# *Step 5: Model Tuning and Hyperparameter Optimization*\n",
    "\n",
    "## 1. Install and Import Required Libraries\n",
    "- **TensorFlow & Keras**: Used for building and training the deep learning model.\n",
    "- **Keras Tuner (`kt`)**: For hyperparameter tuning.\n",
    "- **Pandas & NumPy**: For data handling and numerical computations.\n",
    "- **TF-IDF Vectorizer**: To convert text data into numerical features.\n",
    "- **Sparse Matrices**: Used to efficiently store high-dimensional TF-IDF features.\n",
    "  \n",
    "## 2. Loading and Preprocessing Data Efficiently\n",
    "**Why use chunking?**  \n",
    "- The dataset may be **large**, leading to memory issues.\n",
    "- We use **`chunksize=10000`** to load data in smaller parts.\n",
    "- `on_bad_lines='skip'` ensures problematic rows do not interrupt loading.\n",
    "\n",
    "**Handling Missing Values:**  \n",
    "- Any missing data in `question1` or `question2` is replaced with an **empty string (`''`)**.\n",
    "\n",
    "**Why concatenate questions?**  \n",
    "- Instead of processing `question1` and `question2` separately, we **merge them into a single string** for each pair before applying TF-IDF.\n",
    "\n",
    "**TF-IDF Feature Extraction:**  \n",
    "- **`max_features=2000`** limits vocabulary size to save memory.\n",
    "\n",
    "## 3. Splitting Data into Training & Testing Sets\n",
    "\n",
    "- **80% training** and **20% testing** split.\n",
    "- Helps evaluate model performance on unseen data.\n",
    "- We also prepare `X_final_test` for making predictions on the actual test dataset.\n",
    "\n",
    "## 4. Building a Tunable Deep Learning Model\n",
    "\n",
    "We use **Keras Tuner** to optimize hyperparameters for our neural network.\n",
    "\n",
    "**Model Architecture**:\n",
    "- **1st Dense Layer**:  \n",
    "  - Units: Tuned between **32 to 128**.\n",
    "  - Activation: **ReLU**.\n",
    "- **Dropout Layer**:\n",
    "  - Dropout rate: Tuned between **0.2 to 0.5** (to prevent overfitting).\n",
    "- **Final Output Layer**:\n",
    "  - **1 neuron** with **Sigmoid Activation** for binary classification.\n",
    "\n",
    "**Optimizers**:\n",
    "- We test between **'adam'**, **'rmsprop'**, and **'sgd'**.\n",
    "\n",
    "**Loss Function**:\n",
    "- `binary_crossentropy`: Suitable for binary classification.\n",
    "\n",
    "**Evaluation Metric**:\n",
    "- **Accuracy**: To measure model performance.\n",
    "\n",
    "## 5. Hyperparameter Tuning with Keras Tuner\n",
    "\n",
    "- We use **RandomizedSearch** to explore different model configurations efficiently.\n",
    "**Key Parameters Tuned**:\n",
    "  - **Number of Neurons** (`dense_units`): Between **32 and 128**.\n",
    "  - **Dropout Rate** (`dropout_1`): Between **0.2 and 0.5**.\n",
    "  - **Optimizer**: Choice between **'adam'**, **'rmsprop'**, and **'sgd'**.\n",
    "- **Max Trials = 5**: Limits excessive computation.\n",
    "- **Executions per Trial = 1**: Runs each configuration once.\n",
    "\n",
    "## 6. Efficient Sparse Data Handling in TensorFlow\n",
    "\n",
    "**Why use Sparse Tensors?**  \n",
    "- TF-IDF produces **high-dimensional** but **sparse** matrices.\n",
    "- Converting to **SparseTensor** reduces memory usage.\n",
    "- `tf.sparse.reorder(X_sparse_tensor)`: Ensures correct ordering of indices before training.\n",
    "\n",
    "**Custom Function (`sparse_to_dataset`)**:\n",
    "- Converts sparse matrices into **TensorFlow datasets**.\n",
    "- Uses **batching** (`batch_size=64`) to speed up training.\n",
    "\n",
    "## 7. Training the Best Tuned Model\n",
    "\n",
    "- After tuning, we extract the **best hyperparameters**.\n",
    "- The model is **rebuilt** with optimal settings.\n",
    "- Trained for **10 epochs** on **efficiently loaded sparse data**.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "- Improved accuracy with the best parameter combination.\n",
    "- Reduced overfitting due to dropout.\n",
    "- Faster training by using **TF datasets** and **sparse matrices**.\n",
    "\n",
    "## 8. Making Predictions on the Test Set\n",
    "\n",
    "**Final Step**:\n",
    "- Convert test set to **sparse format**.\n",
    "- Use the **best trained model** to predict duplicate questions.\n",
    "- Apply a **0.5 probability threshold** for classification.\n",
    "- Save results as `test_predictions_tuned.csv`.\n",
    "\n",
    "**Final Output**:  \n",
    "- The model generates predictions and **saves them in a CSV file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45824932-7ec7-447c-b078-2372dff50990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (2.18.0)\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (1.4.7)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sajjad siam\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb6e81d-efaa-481d-8374-8590a44d940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_results\\quora_duplicate_tuning\\tuner0.json\n",
      "Epoch 1/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - accuracy: 0.7172 - loss: 0.5467 - val_accuracy: 0.7525 - val_loss: 0.4962\n",
      "Epoch 2/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.7570 - loss: 0.4904 - val_accuracy: 0.7646 - val_loss: 0.4777\n",
      "Epoch 3/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 7ms/step - accuracy: 0.7730 - loss: 0.4672 - val_accuracy: 0.7732 - val_loss: 0.4648\n",
      "Epoch 4/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.7848 - loss: 0.4486 - val_accuracy: 0.7779 - val_loss: 0.4568\n",
      "Epoch 5/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 7ms/step - accuracy: 0.7938 - loss: 0.4341 - val_accuracy: 0.7816 - val_loss: 0.4512\n",
      "Epoch 6/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.8022 - loss: 0.4207 - val_accuracy: 0.7851 - val_loss: 0.4472\n",
      "Epoch 7/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.8086 - loss: 0.4095 - val_accuracy: 0.7864 - val_loss: 0.4446\n",
      "Epoch 8/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.8143 - loss: 0.4008 - val_accuracy: 0.7880 - val_loss: 0.4436\n",
      "Epoch 9/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 7ms/step - accuracy: 0.8174 - loss: 0.3916 - val_accuracy: 0.7893 - val_loss: 0.4422\n",
      "Epoch 10/10\n",
      "\u001b[1m5054/5054\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.8222 - loss: 0.3844 - val_accuracy: 0.7906 - val_loss: 0.4424\n",
      "\u001b[1m36654/36654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step\n",
      "Test predictions saved to 'test_predictions_tuned.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load and Preprocess Data\n",
    "chunk_size = 10000  # Read file in chunks\n",
    "train_df = pd.concat(pd.read_csv('train.csv', chunksize=chunk_size, encoding='utf-8', on_bad_lines='skip', low_memory=False), ignore_index=True).fillna('')\n",
    "test_df = pd.concat(pd.read_csv('test.csv', chunksize=chunk_size, encoding='utf-8', on_bad_lines='skip', low_memory=False), ignore_index=True).fillna('')\n",
    "\n",
    "# Convert Text to TF-IDF Features\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(train_df['question1'] + \" \" + train_df['question2'])\n",
    "y = train_df['is_duplicate'].values  # Convert labels to NumPy array\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_final_test = vectorizer.transform(test_df['question1'] + \" \" + test_df['question2'])\n",
    "\n",
    "\n",
    "# Define Tunable Model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_results',\n",
    "    project_name='quora_duplicate_tuning'\n",
    ")\n",
    "\n",
    "\n",
    "# Efficiently Load Sparse Data with Sorted Indices\n",
    "def sparse_to_dataset(X, y, batch_size=64):\n",
    "    indices = np.column_stack(X.nonzero()).astype(np.int64)  \n",
    "    values = X.data.astype(np.float32)  # Get values\n",
    "    shape = X.shape\n",
    "\n",
    "    X_sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=shape)\n",
    "    X_sparse_tensor = tf.sparse.reorder(X_sparse_tensor)  \n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_sparse_tensor, y))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(sparse_to_dataset(X_train, y_train), validation_data=sparse_to_dataset(X_test, y_test), epochs=5)\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(sparse_to_dataset(X_train, y_train), validation_data=sparse_to_dataset(X_test, y_test), epochs=10)\n",
    "\n",
    "\n",
    "# Evaluate and Predict on Test Data\n",
    "num_test_samples = X_final_test.shape[0]  \n",
    "test_predictions = (best_model.predict(sparse_to_dataset(X_final_test, np.zeros(num_test_samples))) > 0.5).astype(int)\n",
    "\n",
    "test_df['is_duplicate'] = test_predictions\n",
    "test_df.to_csv('test_predictions_tuned.csv', index=False)\n",
    "print(\"Test predictions saved to 'test_predictions_tuned.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7b56d-cb40-4fa1-9f3d-795c15f0b930",
   "metadata": {},
   "source": [
    "## Function: Convert Sparse Data to Dataset\n",
    "**Why use a generator?**\n",
    "- The test dataset is **sparse**, so converting it to **dense format** would consume excessive memory.\n",
    "- Instead, we **convert and yield one row at a time** to keep memory usage low.\n",
    "\n",
    "**Batch Processing**\n",
    "- The dataset is batched (`batch_size=64`) to enable efficient GPU/CPU processing.\n",
    "- **`.repeat()` was removed** because test data should not be repeated.\n",
    "\n",
    "**Goal**:  \n",
    "Convert the **sparse TF-IDF matrix** into a format that the **TensorFlow model can process efficiently**.\n",
    "\n",
    "## Load Test Data (Ensure X_final_test is available)\n",
    "**Why check for `X_final_test`?**\n",
    "- If the variable is **not defined**, the script will raise an error.\n",
    "- This prevents runtime crashes due to missing data.\n",
    "\n",
    "**Loading Test IDs**\n",
    "- We read `test.csv` to extract `test_id` values.\n",
    "- These are used in the final submission file.\n",
    "\n",
    "**Goal**:  \n",
    "Ensure that the test data is **loaded properly** before making predictions.\n",
    "\n",
    "## Make Predictions using the Trained Model\n",
    "**Why check for `best_model`?**\n",
    "- Ensures the trained model is **loaded before prediction**.\n",
    "- If `best_model` is missing, an error is raised.\n",
    "\n",
    "**Why `steps=test_steps`?**\n",
    "- Ensures predictions match the number of batches in the dataset.\n",
    "- Uses `.shape[0] // 64` (batch size) to determine the number of steps.\n",
    "\n",
    "**Converting Predictions to Binary Labels**\n",
    "- The model outputs **probabilities** between `0` and `1`.\n",
    "- We apply **thresholding** (`> 0.5`) to classify predictions as **0 (Not Duplicate) or 1 (Duplicate)**.\n",
    "\n",
    "**Goal**:  \n",
    "Use the trained model to **predict duplicate questions** in the test set.\n",
    "\n",
    "## Prepare Submission File\n",
    "**Ensure `test_id` Count Matches Predictions**\n",
    "- Some test samples may be missing or skipped.\n",
    "- We use `test_ids[:len(test_predictions)]` to **ensure alignment**.\n",
    "\n",
    "**Save as `test_predictions_tuned.csv`**\n",
    "- The file is saved in CSV format.\n",
    "- This allows submission to **Kaggle or any evaluation platform**.\n",
    "\n",
    "**Goal**:  \n",
    "Generate a well-formatted submission file containing **test IDs and their corresponding predictions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa34657-cf5c-4b3a-8e6f-609368cba0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36653/36653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1259s\u001b[0m 34ms/step\n",
      "Test predictions saved to 'test_predictions_tuned.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Function: Convert Sparse Data to Dataset\n",
    "def sparse_to_dataset(X, batch_size=64):\n",
    "    \"\"\" Convert sparse matrix to a TensorFlow dataset without converting to dense. \"\"\"\n",
    "    def generator():\n",
    "        for i in range(X.shape[0]): \n",
    "            yield X[i].toarray().flatten()  # Convert only one row at a time\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=tf.TensorSpec(shape=(X.shape[1],), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    return dataset.batch(batch_size)  # Removed .repeat() for test data\n",
    "\n",
    "\n",
    "# Load Test Data (Ensure X_final_test is available)\n",
    "try:\n",
    "    X_final_test.shape  # Check if it's defined\n",
    "except NameError:\n",
    "    raise ValueError(\"X_final_test is not defined. Please load the test data.\")\n",
    "\n",
    "# Load sample submission file to get test IDs\n",
    "sample_submission = pd.read_csv(\"test.csv\")\n",
    "test_ids = sample_submission['test_id'].values\n",
    "\n",
    "\n",
    "# Make Predictions using the Trained Model\n",
    "\n",
    "test_dataset = sparse_to_dataset(X_final_test)  # Convert test data to dataset\n",
    "\n",
    "# Ensure `best_model` is defined\n",
    "try:\n",
    "    best_model  # Check if the model exists\n",
    "except NameError:\n",
    "    raise ValueError(\"best_model is not defined. Please load the trained model.\")\n",
    "\n",
    "# Predict using the trained model\n",
    "test_steps = X_final_test.shape[0] // 64  \n",
    "test_predictions = best_model.predict(test_dataset, steps=test_steps)\n",
    "test_predictions = (test_predictions > 0.5).astype(int)  # Convert to binary output\n",
    "test_predictions = test_predictions.flatten()  # Ensure correct shape\n",
    "\n",
    "\n",
    "# Prepare Submission File\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"test_id\": test_ids[:len(test_predictions)],  # Ensure ID count matches predictions\n",
    "    \"is_duplicate\": test_predictions\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission_csv_path = \"test_predictions_tuned.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "print(f\"Test predictions saved to '{submission_csv_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f087a-2c69-4451-8433-63bb494a6159",
   "metadata": {},
   "source": [
    " In this step, we convert the sparse test data into a format suitable for our trained model, make predictions, and generate a submission file.\n",
    "\n",
    "## Key Steps:\n",
    "### 1. Convert Sparse Data to TensorFlow Dataset\n",
    "- Uses a generator function to avoid memory issues.\n",
    "- Batches the data for efficient processing.\n",
    "  \n",
    "### 2. Load and Verify Test Data:\n",
    "- Ensures X_final_test is available.\n",
    "- Retrieves test_id values for submission.\n",
    "\n",
    "### 3. Convert Test Data to TensorFlow Dataset:\n",
    "- Prepares the test data for model inference.\n",
    "  \n",
    "### 4. Make Predictions with the Trained Model:\n",
    "- Ensures best_model is available before inference.\n",
    "- Converts probabilities to binary labels (0 or 1).\n",
    "  \n",
    "### 5. Prepare and Save Submission File:\n",
    "- Creates a DataFrame with test_id and is_duplicate.\n",
    "- Saves predictions as test_predictions_tuned.csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
