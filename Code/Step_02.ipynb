{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72510658-bb01-4378-b024-f6e258b41630",
   "metadata": {},
   "source": [
    "# *Step 02: Text Preprocessing*\n",
    "### 1. Import Required Libraries for Text Preprocessing\n",
    "- In this section, we import necessary libraries for handling text data.\n",
    "- These tools help transform raw text into structured numerical representations for model training.\n",
    "\n",
    "### 2. Loading and Handling Missing Data\n",
    "- If any question pair has a missing value, we drop that row.\n",
    "- This ensures that only complete question pairs are used for preprocessing.\n",
    "\n",
    "### 3. Text Preprocessing Pipeline\n",
    "We define a function `preprocess_text()` to clean and prepare text data for modeling.\n",
    "\n",
    "1. **Lowercasing**: Converts all text to lowercase for consistency.\n",
    "2. **Removing Special Characters**: Removes punctuation, numbers, and symbols.\n",
    "3. **Tokenization**: Splits text into individual words (tokens).\n",
    "4. **Removing Stopwords**: Eliminates common words (e.g., \"the\", \"is\", \"and\") to reduce noise.\n",
    "5. **Lemmatization**: Converts words to their root form (e.g., \"running\" → \"run\").\n",
    "\n",
    "This transformation ensures that the text is standardized and ready for further processing.\n",
    "\n",
    "### 4. Training Word2Vec for Word Embeddings\n",
    "We train a `Word2Vec` model using the cleaned text data:\n",
    "\n",
    "- `vector_size=100`: Each word is represented as a **100-dimensional vector**.\n",
    "- `window=5`: Context window size for capturing relationships between words.\n",
    "- `min_count=1`: Includes all words appearing at least once in the corpus.\n",
    "- `workers=4`: Uses multi-threading for faster training.\n",
    "\n",
    "Word embeddings help capture **semantic relationships** between words, improving the model's ability to understand meaning.\n",
    "\n",
    "### 5. Feature Extraction using TF-IDF\n",
    "To convert text into numerical features, we apply **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "\n",
    "- **TF** measures how often a word appears in a document.\n",
    "- **IDF** reduces the importance of frequently occurring words.\n",
    "\n",
    "This technique helps capture **important keywords** while ignoring common words that don’t add value.\n",
    "\n",
    "### 6. Display Results\n",
    "Finally, we print a **sample of the cleaned text** and check the **shape of the TF-IDF matrix**.\n",
    "\n",
    "- The cleaned text shows how preprocessing transformed raw questions.\n",
    "- The TF-IDF matrix shape gives the total number of features extracted.\n",
    "\n",
    "This ensures that our preprocessing steps were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4806ee8e-dccc-419e-862b-e57307fe88b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Processed Text:\n",
      "                                     question1_clean  \\\n",
      "0  [step, step, guide, invest, share, market, india]   \n",
      "1              [story, kohinoor, koh, noor, diamond]   \n",
      "2  [increase, speed, internet, connection, using,...   \n",
      "3                          [mentally, lonely, solve]   \n",
      "4  [one, dissolve, water, quikly, sugar, salt, me...   \n",
      "\n",
      "                                     question2_clean  \n",
      "0         [step, step, guide, invest, share, market]  \n",
      "1  [would, happen, indian, government, stole, koh...  \n",
      "2         [internet, speed, increased, hacking, dns]  \n",
      "3  [find, remainder, math, 23, 24, math, divided,...  \n",
      "4                [fish, would, survive, salt, water]  \n",
      "\n",
      "TF-IDF Matrix Shape: (404287, 86152)\n",
      "Text Preprocessing, TF-IDF, and Word Embeddings completed successfully!\n"
     ]
    }
   ],
   "source": [
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "## Load Dataset\n",
    "file_path = \"train.csv\"  # Ensure this file is in the same directory\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#  Handle Missing Values\n",
    "df = df.dropna(subset=['question1', 'question2'])\n",
    "\n",
    "##Text Preprocessing Pipeline\n",
    "\n",
    "# Initialize NLP Tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text by tokenizing, lowercasing, removing stopwords & lemmatizing.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Lowercasing\n",
    "        text = re.sub(r'[^a-zA-Z0-9]', ' ', text)  # Remove special characters & punctuation\n",
    "        words = word_tokenize(text)  # Tokenization\n",
    "        words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "        return words\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply Preprocessing function to both question columns\n",
    "df['question1_clean'] = df['question1'].apply(preprocess_text)\n",
    "df['question2_clean'] = df['question2'].apply(preprocess_text)\n",
    "\n",
    "## Train Word2Vec Model for Word Embeddings\n",
    "\n",
    "# Combine sentences from both questions for training\n",
    "sentences = df['question1_clean'].tolist() + df['question2_clean'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save trained model\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "## Feature Extraction using TF-IDF\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "df['combined_text'] = df['question1'].fillna('') + ' ' + df['question2'].fillna('')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "## Display Results\n",
    "\n",
    "# Print sample processed text, shape of TF-IDF matrix & Completion message\n",
    "print(\"Sample Processed Text:\")\n",
    "print(df[['question1_clean', 'question2_clean']].head())\n",
    "print(\"\\nTF-IDF Matrix Shape:\", X_tfidf.shape)\n",
    "print(\"Text Preprocessing, TF-IDF, and Word Embeddings completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152eedd1-6ffb-4482-9d6f-75d58e22dbdd",
   "metadata": {},
   "source": [
    "# Summary of Text Preprocessing\n",
    "\n",
    "### Key Preprocessing Steps:\n",
    "1. **Loading & Cleaning Data**: Dropped missing values.\n",
    "2. **Text Cleaning**:\n",
    "   - Lowercased text.\n",
    "   - Removed special characters and stopwords.\n",
    "   - Tokenized and lemmatized words.\n",
    "3. **Word Embeddings (Word2Vec)**: Trained a model to capture word relationships.\n",
    "4. **Feature Extraction (TF-IDF)**: Converted text into numerical form for model training.\n",
    "\n",
    "### Final Results:\n",
    "- Text data is now structured for machine learning.\n",
    "- The **TF-IDF matrix shape** indicates the number of extracted features.\n",
    "- **Word2Vec embeddings** are saved for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87becfc3-675b-4ef9-9046-729e2cf9f5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
